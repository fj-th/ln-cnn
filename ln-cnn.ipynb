{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import *\n",
    "from numpy import *\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embed_size=32, max_length=40, filter_sizes=(2, 3, 4, 5), filter_num=64):\n",
    "    inp = Input(shape=(max_length,))\n",
    "    emb = Embedding(0xffff, embed_size,embeddings_regularizer=regularizers.l1(0.01))(inp)\n",
    "    emb_ex = Reshape((max_length, embed_size, 1))(emb)\n",
    "    convs = []\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        conv = Conv2D(filter_num, (filter_size, embed_size), activation=\"relu\")(emb_ex)\n",
    "        pool = MaxPooling2D(pool_size=(max_length - filter_size + 1, 1))(conv)\n",
    "        convs.append(pool)\n",
    "\n",
    "    convs_merged = Concatenate()(convs)\n",
    "    reshape = Reshape((filter_num * len(filter_sizes),))(convs_merged)\n",
    "    fc1 = Dense(32, activation=\"relu\")(reshape)\n",
    "    bn1 = BatchNormalization()(fc1)\n",
    "    fc2 = Dense(1, activation='sigmoid')(bn1)\n",
    "    model = Model(input=inp, output=fc2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath, targets, max_length=20, min_length=1):\n",
    "    titles = []\n",
    "    tmp_comments = []\n",
    "    with codecs.open(filepath, 'r', 'utf-8', 'ignore') as f:\n",
    "        for l in f:\n",
    "            label_id, title = l.split(\"\\t\", 1)\n",
    "\n",
    "            if label_id != \"0\" and label_id!=\"1\":\n",
    "                continue\n",
    "            \n",
    "            title = [ord(x) for x in title]\n",
    "            # 長い部分は打ち切り\n",
    "            title = title[:max_length]\n",
    "            title_len = len(title)\n",
    "\n",
    "            if title_len < max_length:\n",
    "\n",
    "                title += ([0] * (max_length - title_len))\n",
    "\n",
    "            titles.append((int(label_id), title))\n",
    "\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(inputs, targets, batch_size=1000, epoch_count=1, max_length=20, model_filepath=\"model.h5\", learning_rate=0.0005):\n",
    "\n",
    "    \n",
    "    start = learning_rate\n",
    "    stop = learning_rate * 0.001\n",
    "    learning_rates = np.linspace(start, stop, epoch_count)\n",
    "\n",
    "    # モデル作成\n",
    "    model = create_model(max_length=max_length)\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #es_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "\n",
    "    \n",
    "    # 学習\n",
    "    history=model.fit(inputs, targets,\n",
    "              nb_epoch=epoch_count,\n",
    "              batch_size=batch_size,\n",
    "              verbose=1,\n",
    "              validation_split=0.2,\n",
    "              shuffle=True,\n",
    "              callbacks=[\n",
    "                  LearningRateScheduler(lambda epoch: learning_rates[epoch]),\n",
    "              ])\n",
    "\n",
    "    # モデルの保存\n",
    "    model.save(model_filepath)\n",
    "    return history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    comments = load_data(\"final-data.txt\",[0])\n",
    "    np.random.shuffle(comments) # shuffle the data (note: validation_split does not shuffle the data before the splitting)\n",
    "\n",
    "    input_values = []\n",
    "    target_values = []\n",
    "    for target_value, input_value in comments:\n",
    "        input_values.append(input_value)\n",
    "        target_values.append(target_value)\n",
    "    input_values = np.array(input_values)\n",
    "    target_values = np.array(target_values)\n",
    "    history=train(input_values, target_values, epoch_count=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.ylim([0.8,1.01])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.ylim([0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ラノベっぽくない: 1.44072175026 %\n"
     ]
    }
   ],
   "source": [
    "def predict(comments, model_filepath=\"model.h5\"):\n",
    "    model = load_model(model_filepath)\n",
    "    ret = model.predict(comments)\n",
    "    return ret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raw_comment = \"銀河ヒッチハイク・ガイド\"\n",
    "    comment = [ord(x) for x in raw_comment]\n",
    "    comment = comment[:20]\n",
    "    if len(comment) < 20:\n",
    "        comment += ([0] * (20 - len(comment)))\n",
    "    ret = predict(np.array([comment]))\n",
    "    predict_result = ret[0][0]\n",
    "    if predict_result > 0.5:\n",
    "        print(\"ラノベっぽくない:\", 100-predict_result * 100,\"%\")\n",
    "    else:\n",
    "        print(\"ラノベっぽい:\", 100-predict_result * 100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
